mkdir -p /tmp/Abhinav_DATA1115331/raw_data/runBB/
mkdir -p /tmp/Abhinav_DATA1115331/models/
mkdir -p /tmp/Abhinav_DATA1115331/hyperparameter_tuning/cnn/




############################################################################## 


           Comment: Binary simulations predictor: Predicting the frequency and drift. 
Best model is under:/tmp/Abhinav_DATA1115331/models/f_predict_cnn_1115331_            


 ############################################################################## 


 
rsync -Pav -q /hercules/scratch/atya/BinaryML/raw_data/runBB/train_data_chunk.npy /tmp/Abhinav_DATA1115331/raw_data/runBB/
rsync -Pav -q /hercules/scratch/atya/BinaryML/raw_data/runBB/val_data_chunk.npy /tmp/Abhinav_DATA1115331/raw_data/runBB/
rsync -Pav -q /hercules/scratch/atya/BinaryML/raw_data/runBB/test_labels_chunk.npy /tmp/Abhinav_DATA1115331/raw_data/runBB/
rsync -Pav -q /hercules/scratch/atya/BinaryML/raw_data/runBB/train_labels_chunk.npy /tmp/Abhinav_DATA1115331/raw_data/runBB/
rsync -Pav -q /hercules/scratch/atya/BinaryML/raw_data/runBB/test_data_chunk.npy /tmp/Abhinav_DATA1115331/raw_data/runBB/
rsync -Pav -q /hercules/scratch/atya/BinaryML/raw_data/runBB/val_labels_chunk.npy /tmp/Abhinav_DATA1115331/raw_data/runBB/
rsync -Pav -q /hercules/scratch/atya/BinaryML/hyperparameter_tuning/cnn/* /tmp/Abhinav_DATA1115331/hyperparameter_tuning/cnn/
mkdir -p /hercules/scratch/atya/BinaryML/hyperparameter_tuning/cnn/
List of dictionaries not found, creating all search combinations
Created all search combinations in 5.653616905212402 seconds
rsync -q -Pav /tmp/Abhinav_DATA1115331/hyperparameter_tuning/cnn/list_of_dicts.json /hercules/scratch/atya/BinaryML/hyperparameter_tuning/cnn/ 
index 0
param_dict {'num_cnn_layers': 4, 'num_deep_layers': 3, 'initial_learning_rate': 0.0005, 'decay_rate': 0.9, 'batch_size': 1200, 'index': 0, 'deep_layer_size': '[64, 128, 256]', 'epochs': 20000, 'patience': 400, 'input_shape': (400, 1), 'padding': 'same', 'dilation': False, 'conv1d_filters': '[64, 128, 256, 512]', 'conv1d_kernel_size': '[5, 7, 7, 7]', 'batch_normalization': False}
param_dict {'num_deep_layers': 6, 'initial_learning_rate': 0.00115, 'decay_rate': 0.95, 'batch_size': 400, 'num_cnn_layers': 10, 'padding': 'valid', 'dilation': True, 'batch_normalization': True, 'index': 12, 'deep_layer_size': '[912, 544, 1528, 1376, 1424, 1360]', 'conv1d_filters': '[1496, 1360, 1224, 1088, 952, 816, 680, 544, 408, 272]', 'conv1d_kernel_size': '[6, 12, 18, 24, 30, 36, 42, 48, 54, 60]', 'dilation_rate_size': '[44, 40, 36, 32, 28, 24, 20, 16, 12, 8]', 'epochs': 20000, 'patience': 400, 'input_shape': (400, 1)}
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 400, 1)]          0         
_________________________________________________________________
conv1d (Conv1D)              (None, 400, 64)           384       
_________________________________________________________________
activation (Activation)      (None, 400, 64)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 400, 128)          57472     
_________________________________________________________________
activation_1 (Activation)    (None, 400, 128)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 400, 256)          229632    
_________________________________________________________________
activation_2 (Activation)    (None, 400, 256)          0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 400, 512)          918016    
_________________________________________________________________
activation_3 (Activation)    (None, 400, 512)          0         
_________________________________________________________________
flatten (Flatten)            (None, 204800)            0         
_________________________________________________________________
dense (Dense)                (None, 64)                13107264  
_________________________________________________________________
dense_1 (Dense)              (None, 128)               8320      
_________________________________________________________________
dense_2 (Dense)              (None, 256)               33024     
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 257       
=================================================================
Total params: 14,354,369
Trainable params: 14,354,369
Non-trainable params: 0
_________________________________________________________________
